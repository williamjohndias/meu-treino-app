# üöÄ Instalar Ollama - 100% Gratuito e Sem Limites!

## ‚úÖ Por que Ollama?

- ‚úÖ **100% Gratuito** - Sem custos, sem cart√£o de cr√©dito
- ‚úÖ **Sem limites de tokens** - Use quanto quiser!
- ‚úÖ **Funciona offline** - N√£o precisa de internet ap√≥s instalar os modelos
- ‚úÖ **Privacidade total** - Tudo roda no seu computador
- ‚úÖ **Sem API Key** - N√£o precisa de cadastros ou chaves
- ‚úÖ **Sem limites de requisi√ß√µes** - Use √† vontade!

---

## üì• Passo 1: Instalar Ollama

1. Acesse: **https://ollama.com/download**
2. Baixe a vers√£o para **Windows**
3. Execute o instalador (`OllamaSetup.exe`)
4. Siga as instru√ß√µes de instala√ß√£o
5. Ollama ser√° instalado e iniciado automaticamente

**Tempo estimado**: 2-3 minutos

---

## üì¶ Passo 2: Baixar um Modelo

Ap√≥s instalar, abra o **PowerShell** ou **Prompt de Comando**.

**‚ö†Ô∏è IMPORTANTE:** Se o comando `ollama` n√£o for reconhecido, use o caminho completo:

```powershell
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.2
```

Ou adicione ao PATH (veja o arquivo `ADICIONAR_OLLAMA_AO_PATH.md`).

Depois, execute um dos comandos abaixo:

### Op√ß√£o 1: Llama 3.2 (Recomendado) ‚≠ê

```bash
ollama pull llama3.2
```

**Caracter√≠sticas:**
- Tamanho: ~2GB
- Boa qualidade e velocidade
- Suporta portugu√™s muito bem
- Equil√≠brio perfeito entre qualidade e performance

**Tempo de download**: 5-10 minutos (dependendo da internet)

### Op√ß√£o 2: Mistral (Mais R√°pido)

```bash
ollama pull mistral
```

**Caracter√≠sticas:**
- Tamanho: ~4GB
- Muito r√°pido
- Excelente qualidade
- √ìtimo para respostas r√°pidas

**Tempo de download**: 10-15 minutos

### Op√ß√£o 3: Phi-3 (Mais Leve)

```bash
ollama pull phi3
```

**Caracter√≠sticas:**
- Tamanho: ~2GB
- Leve e eficiente
- R√°pido
- Ideal para computadores com menos RAM

**Tempo de download**: 5-10 minutos

### Op√ß√£o 4: Llama 3.1 (Mais Poderoso)

```bash
ollama pull llama3.1:8b
```

**Caracter√≠sticas:**
- Tamanho: ~4.7GB
- Melhor qualidade
- Mais lento, mas mais preciso
- Para tarefas mais complexas

**Tempo de download**: 10-15 minutos

---

## ‚ñ∂Ô∏è Passo 3: Verificar Instala√ß√£o

Execute no terminal para ver os modelos instalados:

```bash
ollama list
```

Voc√™ deve ver algo como:
```
NAME            ID              SIZE    MODIFIED
llama3.2        8fdf8c56f1...   2.0 GB  2 hours ago
```

---

## üéØ Passo 4: Testar Ollama

Teste se est√° funcionando:

```bash
ollama run llama3.2
```

Digite uma pergunta e veja se responde. Digite `/bye` para sair.

---

## üöÄ Passo 5: Executar a Aplica√ß√£o

Agora voc√™ pode executar:

```bash
python -m streamlit run app.py
```

**A aplica√ß√£o detectar√° automaticamente o Ollama e usar√° o modelo instalado!**

Se voc√™ tiver m√∫ltiplos modelos, a aplica√ß√£o tentar√° usar nesta ordem:
1. `llama3.2` (se dispon√≠vel)
2. `mistral` (se dispon√≠vel)
3. `phi3` (se dispon√≠vel)
4. O primeiro modelo dispon√≠vel na lista

---

## üí° Dicas Importantes

### Primeira Execu√ß√£o
- Pode demorar um pouco na primeira vez que usar cada modelo
- O modelo √© carregado na mem√≥ria RAM
- Ap√≥s carregar, fica mais r√°pido

### Uso de Mem√≥ria
- **Llama 3.2**: Precisa de ~4GB de RAM livre
- **Mistral**: Precisa de ~8GB de RAM livre
- **Phi-3**: Precisa de ~4GB de RAM livre
- **Recomendado**: Ter pelo menos 8GB de RAM total no sistema

### Performance
- Funciona melhor com mais RAM dispon√≠vel
- SSD ajuda no carregamento inicial
- Processador multi-core melhora a velocidade

### M√∫ltiplos Modelos
- Voc√™ pode instalar v√°rios modelos
- Cada modelo ocupa espa√ßo no disco
- Escolha o que melhor se adapta √†s suas necessidades

---

## üÜò Problemas Comuns

### "Ollama n√£o est√° rodando"

**Solu√ß√£o:**
1. Procure por "Ollama" no menu Iniciar do Windows
2. Execute o aplicativo Ollama
3. Ou execute no terminal: `ollama serve`
4. Aguarde alguns segundos e tente novamente

### "Nenhum modelo instalado"

**Solu√ß√£o:**
1. Abra o PowerShell ou Prompt de Comando
2. Execute: `ollama pull llama3.2` (ou outro modelo)
3. Aguarde o download completar
4. Verifique com: `ollama list`

### "Erro de conex√£o" ou "Connection refused"

**Solu√ß√£o:**
1. Verifique se Ollama est√° rodando (procure no gerenciador de tarefas)
2. Certifique-se de que a porta 11434 n√£o est√° bloqueada pelo firewall
3. Reinicie o Ollama
4. Tente executar: `ollama serve` manualmente

### "Out of memory" ou "Mem√≥ria insuficiente"

**Solu√ß√£o:**
1. Feche outros programas que usam muita RAM
2. Use um modelo menor (phi3 ou llama3.2 ao inv√©s de mistral)
3. Reinicie o computador para liberar mem√≥ria
4. Considere adicionar mais RAM ao sistema

### "Modelo muito lento"

**Solu√ß√£o:**
1. Use um modelo menor (phi3 √© o mais r√°pido)
2. Feche outros programas
3. Certifique-se de ter RAM suficiente
4. Primeira execu√ß√£o sempre √© mais lenta (modelo carrega na mem√≥ria)

---

## üìä Compara√ß√£o de Modelos

| Modelo | Tamanho | RAM Necess√°ria | Velocidade | Qualidade | Portugu√™s |
|--------|---------|----------------|------------|-----------|-----------|
| **llama3.2** | ~2GB | 4GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **mistral** | ~4GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **phi3** | ~2GB | 4GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **llama3.1:8b** | ~4.7GB | 8GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Recomenda√ß√£o para este projeto**: **llama3.2** - Melhor equil√≠brio!

---

## üîÑ Comandos √öteis

### Listar modelos instalados
```bash
ollama list
```

### Remover um modelo
```bash
ollama rm nome_do_modelo
```

### Ver informa√ß√µes de um modelo
```bash
ollama show llama3.2
```

### Atualizar Ollama
```bash
# Windows: Baixe e instale a vers√£o mais recente do site
# https://ollama.com/download
```

### Parar Ollama
```bash
# Feche o aplicativo Ollama ou use o Gerenciador de Tarefas
```

---

## ‚úÖ Checklist de Instala√ß√£o

- [ ] Baixei e instalei o Ollama
- [ ] Ollama est√° rodando (aparece na bandeja do sistema)
- [ ] Baixei pelo menos um modelo (`ollama pull llama3.2`)
- [ ] Verifiquei com `ollama list` que o modelo est√° instalado
- [ ] Testei com `ollama run llama3.2`
- [ ] Executei `python -m streamlit run app.py`
- [ ] A aplica√ß√£o detectou o Ollama automaticamente

---

## üéâ Pronto!

Agora voc√™ tem uma solu√ß√£o **100% gratuita, sem limites de tokens e sem necessidade de API key**!

A aplica√ß√£o detectar√° automaticamente o Ollama e usar√° o modelo instalado. Voc√™ pode usar √† vontade sem se preocupar com limites ou custos!

---

## üìû Precisa de Ajuda?

Se tiver problemas:
1. Verifique se Ollama est√° rodando
2. Verifique se h√° modelos instalados: `ollama list`
3. Consulte a documenta√ß√£o: https://ollama.com/docs
4. Reinicie o Ollama e tente novamente

**Boa sorte com seu projeto!** üöÄ

